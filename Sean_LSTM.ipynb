{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
    "\n",
    "\n",
    "# NOTE:\n",
    "# 1) Split data into train and test\n",
    "# 2) Training data turns into a special input dataset of shape (size, frames=t_lookback*500//sample_reduction, features=32)\n",
    "#       Because alot of the time there is 0's meaning no event, we only want to train where there is an event \n",
    "#       (note: this may have the effect of increasing false positives as we are focussing on when events occur)\n",
    "#       We downsample and look back a fixed amount of time.\n",
    "# 3) No downsample for test data. Look back a fixed amount of time and use a set number of frames spaced by the sample_reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" READING THE DATA \"\"\"\n",
    "DATA_PATH = 'C:\\\\Users\\\\Sean\\\\Desktop\\\\AI4E_A3' # Directory containing all the data\n",
    "# DATA_PATH = 'C:\\\\Users\\\\seanb\\\\Desktop\\\\AI4E\\\\train'\n",
    "DATA_FREQUENCY = 500 # The dataset is sampled at 500 Hz\n",
    "\n",
    "# Sorter function to sort the files in the correct order\n",
    "file_name_sorter = lambda string_list: sorted(string_list, key=lambda s: (int(re.search(r'subj(\\d+)', s).group(1)), int(re.search(r'series(\\d+)', s).group(1)) if re.search(r'series(\\d+)', s) else 0))\n",
    "\n",
    "\n",
    "# Take the first 6 series of each subject to be the training set\n",
    "train_data_files = file_name_sorter(glob(os.path.join(DATA_PATH,'train', 'subj*_series[1-6]_data.csv')))\n",
    "train_filtered_data_files = file_name_sorter(glob(os.path.join(DATA_PATH,'train_filtered', 'subj*_series[1-6]_*.csv')))\n",
    "train_event_files = file_name_sorter(glob(os.path.join(DATA_PATH, 'train', 'subj*_series[1-6]_events.csv')))\n",
    "\n",
    "# Take the 7th and 8th series to be the test set\n",
    "test_data_files = file_name_sorter(glob(os.path.join(DATA_PATH, 'train', 'subj*_series[7-8]_data.csv')))\n",
    "test_filtered_data_files = file_name_sorter(glob(os.path.join(DATA_PATH,'train_filtered', 'subj*_series[7-8]_*.csv')))\n",
    "test_event_files = file_name_sorter(glob(os.path.join(DATA_PATH, 'train', 'subj*_series[7-8]_events.csv')))\n",
    "\n",
    "\n",
    "\n",
    "# Remove the first column and convert to NUMPY array, each row is a sample and each column is a feature.\n",
    "\n",
    "x_train_data_series = [pd.read_csv(file, index_col=None, header=0).iloc[:,1:].to_numpy(dtype=np.float16) for file in train_data_files] # List of the training time series' from each subj_series\n",
    "# x_train_data_series = [pd.read_csv(file, index_col=None, header=0).iloc[:,1:].to_numpy(dtype=np.float16) for file in train_filtered_data_files] # FILTERED\n",
    "y_train_data_series = [pd.read_csv(file, index_col=None, header=0).iloc[:,1:].to_numpy(dtype=np.float16) for file in train_event_files]\n",
    "\n",
    "x_test_data_series = [pd.read_csv(file, index_col=None, header=0).iloc[:,1:].to_numpy(dtype=np.float16) for file in test_data_files]\n",
    "# x_test_data_series = [pd.read_csv(file, index_col=None, header=0).iloc[:,1:].to_numpy(dtype=np.float16) for file in test_filtered_data_files] # FILTERED TODO: whether to use filtered or not\n",
    "y_test_data_series = [pd.read_csv(file, index_col=None, header=0).iloc[:,1:].to_numpy(dtype=np.float16) for file in test_event_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SCALING \"\"\"\n",
    "scaler = RobustScaler()\n",
    "\n",
    "scaler = scaler.fit(np.concatenate(x_train_data_series, axis=0))\n",
    "\n",
    "x_train_data_series = [scaler.transform(series) for series in x_train_data_series]\n",
    "x_test_data_series = [scaler.transform(series) for series in x_test_data_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATING TRAINING DATA FORMAT \"\"\"\n",
    "def generate_input_dataset(x_train_data_series, y_train_data_series, t_lookback=2, sample_reduction=50):\n",
    "    \"\"\" Generate the input dataset for the LSTM model. This is done by looking back a fixed amount of time and downsampling the data.\"\"\"\n",
    "\n",
    "    # t_lookback specifies how many seconds to look back for LSTM\n",
    "    # sample_reduction specifies how much to downsample the data by (e.g. for 10 means 500/10 = 50 Hz, 50 menans 500/50 = 10 Hz). This mimics a reduced sampling rate of EEG data by taking every nth reading\n",
    "\n",
    "    \n",
    "    n_frames = t_lookback * DATA_FREQUENCY # Number of data frames to look back NOT REDUCED\n",
    "    x = [] # Train data\n",
    "    y = [] # Train labels\n",
    "\n",
    "    # Go through each series/events file separately\n",
    "    for series_data, series_events in zip(x_train_data_series, y_train_data_series):\n",
    "        \n",
    "        for i in range(n_frames-1, series_data.shape[0]-1): # Go through all the possible predictable frames (frames that have hisotrical data avaiable)\n",
    "            \n",
    "            if np.sum(series_events[i]) > 0: # If there is an event at this frame\n",
    "\n",
    "                x.append(series_data[i-n_frames+sample_reduction:i+sample_reduction:sample_reduction]) # Append the data\n",
    "                y.append(series_events[i]) # Append the label for the event with the historicla data in mind\n",
    "                \n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x_train, y_train = generate_input_dataset(x_train_data_series, y_train_data_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SHUFFLE THE DATA \"\"\"\n",
    "shuffle_idx = np.random.permutation(x_train.shape[0])\n",
    "x_train = x_train[shuffle_idx]\n",
    "y_train = y_train[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " LSTM3 (CuDNNLSTM)           (None, 64)                25088     \n",
      "                                                                 \n",
      " dropout3 (Dropout)          (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,478\n",
      "Trainable params: 25,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" BUILD THE MODEL \"\"\"\n",
    "model = tf.keras.models.Sequential(layers=[\n",
    "\n",
    "    tf.keras.layers.Input((x_train.shape[1], x_train.shape[2]), name='input'), # Input should be frames*features (e.g. 500*32)\n",
    "\n",
    "    # tf.compat.v1.keras.layers.CuDNNLSTM(units=128, name='LSTM1', return_sequences=True),\n",
    "    # tf.keras.layers.Dropout(0.5, name='dropout1'),\n",
    "\n",
    "    \n",
    "    # tf.compat.v1.keras.layers.CuDNNLSTM(units=128, name='LSTM2', return_sequences=True),\n",
    "    # tf.keras.layers.Dropout(0.5, name='dropout2'),\n",
    "\n",
    "    \n",
    "    tf.compat.v1.keras.layers.CuDNNLSTM(units=64, name='LSTM3'),\n",
    "    tf.keras.layers.Dropout(0.5, name='dropout3'),\n",
    "\n",
    "    tf.keras.layers.Dense(6, activation='sigmoid', name='dense') # Output layer, 6 independent possible events from 0-1\n",
    "\n",
    "    ], name='LSTM_model'\n",
    ")\n",
    "\n",
    "loss=tf.keras.losses.binary_crossentropy # Binary cross-entropy treats each label prediction as a separate binary classification problem.\n",
    "optimizer=tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.AUC(name='auc')]) # AUC - minmise false positives, maximise true positives\n",
    "\n",
    "# Recall is low, meaning its not getting alot of the true positives. This may be expected since if we look at the training data, there are alot of 0's and only a few 1's. This means that the model will be biased towards predicting 0's. This is a problem since we want to predict the 1's. We can try to fix this by weighting the loss function to penalise false negatives more than false positives. This will make the model more likely to predict 1's.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1342/1342 [==============================] - 10s 8ms/step - loss: 0.1387 - accuracy: 0.8240 - precision: 0.8479 - recall: 0.8532 - auc: 0.9811 - val_loss: 0.1196 - val_accuracy: 0.8420 - val_precision: 0.8666 - val_recall: 0.8746 - val_auc: 0.9861\n",
      "Epoch 2/5\n",
      "1342/1342 [==============================] - 10s 7ms/step - loss: 0.1355 - accuracy: 0.8277 - precision: 0.8515 - recall: 0.8575 - auc: 0.9820 - val_loss: 0.1184 - val_accuracy: 0.8428 - val_precision: 0.8745 - val_recall: 0.8704 - val_auc: 0.9864\n",
      "Epoch 3/5\n",
      "1342/1342 [==============================] - 10s 7ms/step - loss: 0.1327 - accuracy: 0.8314 - precision: 0.8546 - recall: 0.8611 - auc: 0.9827 - val_loss: 0.1141 - val_accuracy: 0.8433 - val_precision: 0.8759 - val_recall: 0.8783 - val_auc: 0.9875\n",
      "Epoch 4/5\n",
      "1342/1342 [==============================] - 10s 7ms/step - loss: 0.1307 - accuracy: 0.8335 - precision: 0.8571 - recall: 0.8637 - auc: 0.9832 - val_loss: 0.1132 - val_accuracy: 0.8504 - val_precision: 0.8796 - val_recall: 0.8769 - val_auc: 0.9875\n",
      "Epoch 5/5\n",
      "1342/1342 [==============================] - 10s 7ms/step - loss: 0.1284 - accuracy: 0.8365 - precision: 0.8601 - recall: 0.8664 - auc: 0.9838 - val_loss: 0.1147 - val_accuracy: 0.8428 - val_precision: 0.8756 - val_recall: 0.8795 - val_auc: 0.9871\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5, batch_size=1024, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(x_train, y_train)\n",
    "# We want to evaluate the whole time series and potentially get a plot to compare the predicted events with the actual events (noting the different time scales)\n",
    "\n",
    "# plt.figure(figsize=(15,10))\n",
    "# plt.legend([1,2,3,4,5,6], loc='upper left')\n",
    "# plt.plot(y_test_data_series[0])\n",
    "\n",
    "# np.array([x_test_data_series[0][0:5], x_test_data_series[0][5:10]]).shape\n",
    "# model.predict(np.array([x_test_data_series[0][0:5], x_test_data_series[0][5:10]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119561\n"
     ]
    }
   ],
   "source": [
    "def prediction_test_time_series(x_test_series, y_test, t_lookback=2, sample_reduction=50):\n",
    "    # Return the time series prediction for a test time series\n",
    "\n",
    "    # Make sure t_lookback is the same as how it was trained\n",
    "    \n",
    "    \n",
    "\n",
    "    n_frames = t_lookback * DATA_FREQUENCY  # Number of data frames to look back \n",
    "    # print(n_frames)\n",
    "\n",
    "    print(x_test_series.shape[0])\n",
    "\n",
    "    x_test = [] # make test be i * n_frames * n_features\n",
    "\n",
    "    # Need to get the x_test into the form of (i, n_frames, n_features) where i represents which frame we are predicting for\n",
    "\n",
    "    for i in range(n_frames, x_test_series.shape[0]):\n",
    "        x_test.append(x_test_series[i-n_frames:i:sample_reduction])\n",
    "\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float16)\n",
    "    return x_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (x_test_data_series[0].shape)\n",
    "\n",
    "# x_train[0].shape\n",
    "\n",
    "tmp = prediction_test_time_series(x_test_data_series[0], y_test_data_series[0])\n",
    "# TODO: Need to do a time series prediction for each of the test data series\n",
    "\n",
    "# TODO: Need to actually train a model as well to see how it goes on the train data series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119061, 5, 32)\n",
      "(119561, 32)\n"
     ]
    }
   ],
   "source": [
    "print(tmp.shape)\n",
    "\n",
    "print(x_test_data_series[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
